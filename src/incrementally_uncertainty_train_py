import numpy as np
from tensorflow.keras.models import clone_model
import matplotlib.pyplot as plt
from time import time

import config as cnf
from prints import printe, printo
from model import hullifier_load, hullifier_compile, hullifier_create, train
from data import load_from_coco, shuffle_data, split_data
from utils import get_dict_from_file, plot_history, find_uncertainty, history_merge, get_predictions_bool


measure_time = True
if measure_time:
    start = time()
def t():
    tm = int(time()-start)
    return f'{tm//60}m {tm%60}s'



# [x] Create NEW hullifier
# [x] Load old data
# [x] split data into original split
# [x] Split train data into smaller splits
# [x] Train on "inital" data
# [] 
# []
# []
# []
# [] compare final result to original result
# [] Seperate data with uncertain images and non uncertain images
# [] Train on only uncertain images and train on only certain images
# [] Compare results
# []


def incrementally_train(X, Y, XT, YT, lr, epochs, image_budget):
    cum_bud = np.cumsum(image_budget)
    incr_epochs = int(epochs * 0.2)
    # eps = np.full(image_budget.shape[0], incr_epochs, dtype=int)
    # eps[0] = epochs # First epoch is initial training phase and is larger

    fig, axs = plt.subplots(1,2)
    model = hullifier_create(X[:3], lr=lr)    
    
    
    

    # The training dataset for the model
    training_X = X[:cum_bud[0]]
    training_Y = Y[:cum_bud[0]]
    h, e = train(
        model,
        training_X,
        training_Y,
        batch_size=50,
        epochs=epochs,
        validation_data=(XT,YT),
    )

    full_hist.append(h)
    skips = total_epochs = 0
    for i, (cb, ib) in enumerate(zip(cum_bud[1:], image_budget[1:])):
        # Find uncertainties in da
        predictions_bool = get_predictions_bool(model, X[cum_bud[i-1]:cb])
        uncertainties = np.empty((ib))
        for i, (image, pred) in enumerate(zip(X[cum_bud[i-1]:cb], predictions_bool)):
            uncertainties[i] = find_uncertainty(image, pred, model)
        # Mask is True for all images that is uncertain
        mask = np.where(np.sum(uncertainties, 1) == 0, True, False)

        new_X = np.delete(X[cum_bud[i-1]:cb], mask, 0)
        new_Y = np.delete(Y[cum_bud[i-1]:cb], mask, 0)
            
        print(f'was uncertain for {np.sum(mask)} images. Was certain for {np.sum(np.invert(mask))} images. New X is shape: {new_X.shape}')

        if new_X.shape[0] == 0:
            skips += 1
            continue
            
        training_X = np.concatenate((training_X, new_X), 0)
        training_Y = np.concatenate((training_Y, new_Y), 0)
        # train on only uncertain images
        h, e = train(
            model,
            training_X,
            training_Y,
            batch_size=50,
            epochs=incr_epochs,
            validation_data=(XT,YT),
        )
        eps.append(e)
        full_hist.append(h)
        
    print(f'Skipped {skips} batche(s) because no images had uncertainties of significance')
    

    cum_e = np.cumsum(eps)
    full_hist = history_merge(full_hist, eps, 'loss', 'binary_accuracy')
    for ce in cum_e:
        axs[0].axvline(ce, linestyle='dashed', linewidth=1,c='red')
        axs[1].axvline(ce, linestyle='dashed', linewidth=1,c='red')
    plot_history(axs[0], total_epochs, full_hist[:2], subfig_text='Loss function')
    plot_history(axs[1], total_epochs, full_hist[2:], subfig_text='Binary accuracy')
    axs[0].set_ylim(top=0.4)
    axs[1].set_ylim(bottom=0.81)

    fig.suptitle(f"Loss and accuracy graph for model trained incrementally")
    fig.tight_layout()
    fig.savefig('../out_imgs/incrementally_uncertainty_learning/'+'incr_incrementally_trained.png')
    if measure_time:
        printo(f'1st fig done after {t()} since start')

# ==========================================================================================================================================

    # Train a new model on only "non-uncertain" images
    fig, axs = plt.subplots(1,2)
    model = hullifier_create(X[:3], lr=lr)    
    
    full_hist = eps = []

    # The training dataset for the model
    training_X = X[:cum_bud[0]]
    training_Y = Y[:cum_bud[0]]
    h, e = train(
        model,
        training_X,
        training_Y,
        batch_size=50,
        epochs=epochs,
        validation_data=(XT,YT),
    )

    full_hist.append(h)
    skips = 0
    eps
    for i, (cb, ib) in enumerate(zip(cum_bud[1:], image_budget[1:])):
        # Find uncertainties in da
        predictions_bool = get_predictions_bool(model, X[cum_bud[i-1]:cb])
        uncertainties = np.empty((ib))
        for i, (image, pred) in enumerate(zip(X[cum_bud[i-1]:cb], predictions_bool)):
            uncertainties[i] = find_uncertainty(image, pred, model)
        # Mask is True for all images that is NOT uncertain
        mask = np.where(np.sum(uncertainties, 1) > 0, True, False)

        new_X = np.delete(X[cum_bud[i-1]:cb], mask, 0)
        new_Y = np.delete(Y[cum_bud[i-1]:cb], mask, 0)
            
        print(f'Was certain for {np.sum(mask)} images. Was Uncertain for {np.sum(np.invert(mask))} images. New X is shape: {new_X.shape}')

        if new_X.shape[0] == 0:
            skips += 1
            continue
            
        training_X = np.concatenate((training_X, new_X), 0)
        training_Y = np.concatenate((training_Y, new_Y), 0)
        # train on only uncertain images
        h, e = train(
            model,
            training_X,
            training_Y,
            batch_size=50,
            epochs=incr_epochs,
            validation_data=(XT,YT),
        )

        full_hist.append(h)
        
    print(f'Skipped {skips} batche(s) because no images had uncertainties of significance')

    # total_epochs =  ((cum_bud.shape[0]) * incr_epochs) + epochs # Delete if line under works
    total_epochs =  cum_e[-1]

    full_hist = history_merge(full_hist, eps, 'loss', 'binary_accuracy')
    for ce in cum_e:
        axs[0].axvline(ce, linestyle='dashed', linewidth=1,c='red')
        axs[1].axvline(ce, linestyle='dashed', linewidth=1,c='red')
    plot_history(axs[0], total_epochs, full_hist[:2], subfig_text='Loss function')
    plot_history(axs[1], total_epochs, full_hist[2:], subfig_text='Binary accuracy')
    axs[0].set_ylim(top=0.4)
    axs[1].set_ylim(bottom=0.81)

    fig.suptitle(f"Loss and accuracy graph for model trained incrementally")
    fig.tight_layout()
    fig.savefig('../out_imgs/incrementally_uncertainty_learning/'+'incr_incrementally_trained.png')
    if measure_time:
        printo(f'2nd fig done after {t()} since start')
# ==========================================================================================================================================
    # Plot regular model normalized
    fig, axs = plt.subplots(1,2)

    ## Plotting regular model normalized
    x = np.linspace(0,1, epochs)
    axs[0].plot(x, h.history['loss'], label='train')
    axs[0].plot(x, h.history['val_loss'], label='test')
    axs[1].plot(x, h.history['binary_accuracy'], label='train')
    axs[1].plot(x, h.history['val_binary_accuracy'], label='test')

    ## Plot incremental model normalized
    x = np.linspace(0, 1, total_epochs)
    axs[0].plot(x, full_hist[0], label='incr. train')
    axs[0].plot(x, full_hist[1], label='incr. test')
    axs[1].plot(x, full_hist[2], label='incr. train')
    axs[1].plot(x, full_hist[3], label='incr. test')

    ## Set axs info
    axs[0].title.set_text('Loss function')
    axs[1].title.set_text('Binary accuracy')
    axs[0].set_xlabel('Total training period')
    axs[1].set_xlabel('Total training period')
    axs[0].grid()
    axs[1].grid()
    axs[0].legend()
    axs[1].legend()
    axs[0].set_ylim(top=0.4)
    axs[1].set_ylim(bottom=0.81)

    fig.suptitle(f"Loss and accuracy graph for\nboth models, normalized")
    fig.tight_layout()
    fig.savefig('../out_imgs/incrementally_uncertainty_learning/'+'incr_both_normalized.png')
    if measure_time:
        printo(f'5th fig done after {t()} since start')

def main():
    # Load parameters from old model
    params = get_dict_from_file(cnf.model_path + 'params.txt')
    # load old model
    params.epochs = int(params.epochs)
    # params.epochs += 5
    # params.epochs += 5
    # params.epochs += 5
    # params.epochs = 5
    
    # Load all the old data
    X, Y = load_from_coco()
    X, Y = shuffle_data(X,Y)
    # Original split
    X,Y,XT,YT = split_data(X,Y,float(params.v_split))    

    image_budget = np.array([
        800, 
        103, # 1
        100, # 2
        100, # 3
        100, # 4
        100, # 5
        100, # 6
        100, # 7
        100, # 8
        100  # 9
        ])

    incrementally_train(X, Y, XT, YT, float(params.lr), params.epochs, image_budget)


if __name__ == "__main__":
    main()
    if measure_time:
        print(f'Total time used: {t()}')